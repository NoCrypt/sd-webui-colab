{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gfKvWAVnz8OB"
      },
      "source": [
        "<details>\n",
        "  <summary><b>Credits</b></summary>\n",
        "\tColab that I've forked and edits:\n",
        "\n",
        "- https://github.com/daswer123/stable-diffusion-colab\n",
        "- https://github.com/acheong08/Diffusion-ColabUI\n",
        "- https://github.com/TheLastBen/fast-stable-diffusion\n",
        "- https://colab.research.google.com/drive/1Iy-xW9t1-OQWhb0hNxueGij8phCyluOh\n",
        "- https://colab.research.google.com/drive/1T8NKPZknFrylmUMAGLsSqUJ9QEgQBs_3\n",
        "- And some chinese colab that I can't share to public :D\n",
        "\n",
        "</details>\n",
        "\n",
        "<fieldset>\n",
        "    <legend>\n",
        "    <h2><b>How to use?</b></h2>\n",
        "    </legend>\n",
        "    \n",
        "1. Edit the settings by checking the checkbox to your liking\n",
        "2. Run the cell by clicking ‚ñ∂Ô∏è button, from top to bottom\n",
        "3. After running the start webui cell, wait for few minutes until it says \"Connected\" then click on the link above it. (not 127.0.0.1)\n",
        "\n",
        "</fieldset>\n",
        "\n",
        "**‚ö†Ô∏è DO NOT USE \"RUN ALL\", I REPEAT, DO NOT USE \"RUN ALL\" ‚ö†Ô∏è** \n",
        "<br>\n",
        "\n",
        "If something is broken:\n",
        "- Fastest way: [dm me](https://lookup.guru/442099748669751297)\n",
        "- Normal way: [issue tab](https://github.com/NoCrypt/sd-webui-colab/issues/new)\n",
        "\n",
        "Useful colabs:\n",
        "- [Mirror and Convert Model](https://colab.research.google.com/drive/1aQ5nXTfLWHhZi7GOfXteLKg5OT1X-aBZ?)\n",
        "- [Merge Model](https://colab.research.google.com/drive/1arV8GLZHTcj1E1_OSWgjTf8UNTocTagC) by andite#8484\n",
        "\n",
        "Have a copy of this colab? [Check updates here.](https://github.com/NoCrypt/sd-webui-colab)\n",
        "\n",
        "## **Latest Changes [02/01]**\n",
        "- Added optional gradio auth for security. Especially when using bore tunnel\n",
        "\n",
        "<br><br>\n",
        "### I'm currently developing a beta colab for further improvements such as much simpler options, much faster startup time, automatically load many stuff at once, and some neat customizations + patches. [Click here to try](https://colab.research.google.com/drive/1wEa-tS10h4LlDykd87TF5zzpXIIQoCmq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DJ7Yzy8Asu3Y"
      },
      "outputs": [],
      "source": [
        "#@title # **üëá Test your GPU runtime**\n",
        "import os\n",
        "from subprocess import getoutput\n",
        "import time\n",
        "\n",
        "def increase_font():\n",
        "  from IPython.display import Javascript\n",
        "  display(Javascript('''\n",
        "  for (rule of document.styleSheets[0].cssRules){\n",
        "    if (rule.selectorText=='body') {\n",
        "      rule.style.fontSize = '45px'\n",
        "      rule.style.fontWeight = '600'\n",
        "      break\n",
        "    }\n",
        "  }\n",
        "  '''))\n",
        "\n",
        "output = getoutput('nvidia-smi --query-gpu=gpu_name --format=csv')\n",
        "increase_font()\n",
        "if \"name\" in output:\n",
        "  print('\u001b[32mGPU Connected! Currently using', output[5:], \"‚úÖ\") \n",
        "  print('You may continue.')\n",
        "else:\n",
        "  print(\"\\033[91mNo GPU accelerator is connected. \\nPlease connect to a GPU Runtime\")\n",
        "print(\"\\033[0m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sBbcB4vwj_jm"
      },
      "outputs": [],
      "source": [
        "#@title # **üëá Install Dependencies (Important)**\n",
        "%cd /content\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import time\n",
        "from subprocess import getoutput\n",
        "from IPython.utils import capture\n",
        "\n",
        "try:\n",
        "  start_colab\n",
        "except:\n",
        "  start_colab = int(time.time())\n",
        "\n",
        "#@markdown ## **Options**\n",
        "run_in_gdrive = False #@param {type:\"boolean\"}\n",
        "#@markdown - run_in_gdrive is a bit experimental, try at your own risk. (it's slow, trust me)\n",
        "#@markdown - xformers were moved to start cell\n",
        "root_dir = \"/content\"\n",
        "if run_in_gdrive:\n",
        "  if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "  !mkdir -p {'/content/gdrive/MyDrive/WebUI'}\n",
        "  root_dir = \"/content/gdrive/MyDrive/WebUI\"\n",
        "\n",
        "\n",
        "# This function will delete the path if exists in drive (reinstall)\n",
        "def rm_drive_path(path):\n",
        "  if run_in_gdrive:\n",
        "    if os.path.exists(root_dir+\"/stable-diffusion-webui/\"+path):\n",
        "      !rm -rf {root_dir}/stable-diffusion-webui/{path}\n",
        "\n",
        "# Install Automatic1111's Webui (or update if already exists)\n",
        "if os.path.exists(root_dir+\"/stable-diffusion-webui\"):\n",
        "  %cd {root_dir}/stable-diffusion-webui/\n",
        "  !git checkout master\n",
        "  !git pull\n",
        "else:\n",
        "  !git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui {root_dir}/stable-diffusion-webui\n",
        "  !mkdir -p {root_dir}/stable-diffusion-webui/models/hypernetworks\n",
        "  %cd {root_dir}/stable-diffusion-webui/\n",
        "\n",
        "commit_hash = \"\" #@param{type:\"string\"}\n",
        "if commit_hash:\n",
        "  with capture.capture_output() as cap:\n",
        "    !git config --global user.email \"you@example.com\"\n",
        "    !git config --global user.name \"Your Name\"\n",
        "    !git stash\n",
        "    !git reset --hard {commit_hash}\n",
        "    !git stash pop\n",
        "\n",
        "# Custom Configuration\n",
        "custom_config = True #@param {type:\"boolean\"}\n",
        "if custom_config:\n",
        "  !wget -q -O styles.csv https://raw.githubusercontent.com/NoCrypt/webui-settings/main/styles.csv\n",
        "  !wget -q -O config.json https://raw.githubusercontent.com/NoCrypt/webui-settings/main/config.json\n",
        "  !wget -q -O ui-config.json https://raw.githubusercontent.com/NoCrypt/webui-settings/main/ui-config.json\n",
        "  !wget -q -O user.css https://raw.githubusercontent.com/NoCrypt/webui-settings/main/user.css\n",
        "\n",
        "# Output images to drive\n",
        "output_to_drive = True #@param{type:\"boolean\"}\n",
        "#@markdown - output_to_drive needs custom_config to be activated\n",
        "if output_to_drive:\n",
        "  if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "  !sed -i 's@\"outdir_txt2img_samples\": \"outputs/txt2img-images\"@\"outdir_txt2img_samples\": \"/content/gdrive/MyDrive/WebUI/outputs/txt2img-images\"@' {root_dir}/stable-diffusion-webui/config.json\n",
        "  !sed -i 's@\"outdir_img2img_samples\": \"outputs/img2img-images\"@\"outdir_img2img_samples\": \"/content/gdrive/MyDrive/WebUI/outputs/img2img-images\"@' {root_dir}/stable-diffusion-webui/config.json\n",
        "  !sed -i 's@\"outdir_extras_samples\": \"outputs/extras-images\"@\"outdir_extras_samples\": \"/content/gdrive/MyDrive/WebUI/outputs/extras-images\"@' {root_dir}/stable-diffusion-webui/config.json\n",
        "  !sed -i 's@\"outdir_txt2img_grids\": \"outputs/txt2img-grids\"@\"outdir_txt2img_grids\": \"/content/gdrive/MyDrive/WebUI/outputs/txt2img-grids\"@' {root_dir}/stable-diffusion-webui/config.json\n",
        "  !sed -i 's@\"outdir_img2img_grids\": \"outputs/img2img-grids\"@\"outdir_img2img_grids\": \"/content/gdrive/MyDrive/WebUI/outputs/img2img-grids\"@' {root_dir}/stable-diffusion-webui/config.json\n",
        "  !sed -i 's@\"outdir_save\": \"log/images\"@\"outdir_save\": \"/content/gdrive/MyDrive/WebUI/outputs/log/images\"@' {root_dir}/stable-diffusion-webui/config.json\n",
        "\n",
        "\n",
        "#@markdown ## <br>**Extension**\n",
        "#@markdown - I'm aware that there's already an extension tab. Think of these as something you want to pre-install for saving time.\n",
        "\n",
        "# Add booru tag suggestion by DominikDoom\n",
        "add_prompt_suggestion = True #@param{type:\"boolean\"}\n",
        "if add_prompt_suggestion:\n",
        "  rm_drive_path('extensions/a1111-sd-webui-tagcomplete')\n",
        "  %cd {root_dir}/stable-diffusion-webui\n",
        "  !git clone https://github.com/DominikDoom/a1111-sd-webui-tagcomplete extensions/a1111-sd-webui-tagcomplete\n",
        "\n",
        "# Add novelai_converter\n",
        "add_novelai_converter = True #@param{type:\"boolean\"}\n",
        "if add_novelai_converter:\n",
        "  rm_drive_path('extensions/novelai-2-local-prompt')\n",
        "  !git clone https://github.com/animerl/novelai-2-local-prompt.git extensions/novelai-2-local-prompt\n",
        "\n",
        "# Add UmiAI\n",
        "add_UmiAI = True #@param{type:\"boolean\"}\n",
        "#@markdown - [UmiAI info](https://www.patreon.com/posts/umi-ai-official-73544634)\n",
        "if add_UmiAI:\n",
        "  rm_drive_path('extensions/UnivAICharGen')\n",
        "  !git clone https://github.com/Klokinator/UnivAICharGen.git extensions/UnivAICharGen\n",
        "\n",
        "# Add Image History\n",
        "add_image_history = True #@param {type:\"boolean\"} \n",
        "if add_image_history:\n",
        "  rm_drive_path('extensions/images-browser')\n",
        "  !git clone https://github.com/yfszzx/stable-diffusion-webui-images-browser extensions/images-browser\n",
        "\n",
        "# Add Dynamic Prompt\n",
        "add_dynamic_prompting = False #@param{type:\"boolean\"}\n",
        "if add_dynamic_prompting:\n",
        "  rm_drive_path('extensions/dynamic-prompts')\n",
        "  !git clone https://github.com/adieyal/sd-dynamic-prompting/ extensions/dynamic-prompts\n",
        "\n",
        "# Add Wildcards\n",
        "add_wildcards = False #@param{type:\"boolean\"}\n",
        "if add_wildcards:\n",
        "  rm_drive_path('extensions/wildcards')\n",
        "  !git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui-wildcards extensions/wildcards\n",
        "\n",
        "\n",
        "#@markdown ##<br>**Experimental üß™**\n",
        "#@markdown *Warning: don't check options below if you dont understand*\n",
        "#@markdown <br> *Don't report if options below is broken. It meant to be.*\n",
        "#@markdown <br> *I'll remove some of the options if it's already been implemented.*\n",
        "#@markdown <br> *Check the code if you wanna know what each option does*\n",
        "\n",
        "pr3962_dynamic_thresholding = False #@param {type:\"boolean\"} \n",
        "using_experimental = False\n",
        "\n",
        "if pr3962_dynamic_thresholding:\n",
        "  !git config --global user.email \"why@not.gg\"\n",
        "  !git config --global user.name \"idk how2use git\"\n",
        "  !git checkout -b experimental_thingy\n",
        "  using_experimental = True\n",
        "\n",
        "#https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/3962\n",
        "if pr3962_dynamic_thresholding:\n",
        "  !git fetch origin pull/3962/head:pr3962\n",
        "  !git merge --no-commit pr3962\n",
        "\n",
        "\n",
        "print(\"\\033[0m\") #Reset color, git tends to leaking red color to terminal which can make ppl think it's an error\n",
        "\n",
        "#@markdown ####<br>**Forked from [fast-stable-diffusion](https://github.com/TheLastBen/fast-stable-diffusion) :**\n",
        "faster_dep_install = False #@param {type:\"boolean\"} \n",
        "if faster_dep_install:\n",
        "  %cd /content/\n",
        "  for i in range(1,7):\n",
        "      !wget -q \"https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dependencies/Dependencies_AUT.{i}\"\n",
        "      !mv \"Dependencies_AUT.{i}\" \"Dependencies_AUT.7z.00{i}\"\n",
        "  !7z x Dependencies_AUT.7z.001\n",
        "  time.sleep(2)\n",
        "  !cp -r /content/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n",
        "  !rm -r /content/usr\n",
        "  for i in range(1,7):\n",
        "      !rm \"Dependencies_AUT.7z.00{i}\"\n",
        "  !pip install -U -q pillow\n",
        "  !pip uninstall -y -q gradio\n",
        "  using_experimental = True\n",
        "\n",
        "\n",
        "high_concurrency_count = False #@param {type:\"boolean\"} \n",
        "import mmap\n",
        "# /content/stable-diffusion-webui/webui.py\n",
        "if os.path.exists(root_dir+\"/stable-diffusion-webui/webui.py\"):\n",
        "  with open(root_dir+\"/stable-diffusion-webui/webui.py\", 'rb', 0) as file:\n",
        "    s = mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ)\n",
        "    if s.find(b'concurrency_count') != -1:\n",
        "      if not high_concurrency_count:\n",
        "        !sed -i 's@ui.create_ui();shared.demo.queue(concurrency_count=999999,status_update_rate=0.1).*@ui.create_ui()@' {root_dir}/stable-diffusion-webui/webui.py\n",
        "    elif high_concurrency_count:\n",
        "      !sed -i 's@ui.create_ui().*@ui.create_ui();shared.demo.queue(concurrency_count=999999,status_update_rate=0.1)@' {root_dir}/stable-diffusion-webui/webui.py\n",
        "      using_experimental = True\n",
        "  \n",
        "large_model_compatibility = False #@param {type:\"boolean\"} \n",
        "if large_model_compatibility:\n",
        "  !sed -i 's@cmd_opts.lowram else \\\"cpu\\\"@cmd_opts.lowram else \\\"cuda\\\"@' {root_dir}/stable-diffusion-webui/modules/shared.py\n",
        "  using_experimental = True\n",
        "else:\n",
        "  !sed -i 's@cmd_opts.lowram else \\\"cuda\\\"@cmd_opts.lowram else \\\"cpu\\\"@' {root_dir}/stable-diffusion-webui/modules/shared.py\n",
        "\n",
        "# Snippet to self for PR pulling:\n",
        "# interesting_feature = True #@param {type:\"boolean\"} \n",
        "# if interesting_feature:\n",
        "#   !git fetch origin pull/6969/head:something_interesting\n",
        "#   !git checkout something_interesting\n",
        "\n",
        "if using_experimental:\n",
        "  print(\"\\033[93m‚ö†Ô∏è WARNING: EXPERIMENTAL OPTION IS ACTIVE! ‚ö†Ô∏è\\nDO NOT REPORT IF ANYTHING IS BROKEN!\\nThe output will be yellow during startup to mark this\\nTo Remove, Rerun install dep without experimental, but restart runtime is highly recommended\\n\")\n",
        "  \n",
        "%cd {root_dir}/stable-diffusion-webui\n",
        "!COMMANDLINE_ARGS=\"--exit\" REQS_FILE=\"requirements.txt\" python launch.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XS_wXAY3FOLx"
      },
      "outputs": [],
      "source": [
        "#@title # **üëá Install Models**\n",
        "\n",
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "try:\n",
        "  start_colab\n",
        "except:\n",
        "  sys.exit(\"ü´§ YOU HAVEN'T RUN INSTALL DEPENDENCIES CELL\")\n",
        "%cd {root_dir}/stable-diffusion-webui/models/Stable-diffusion/\n",
        "!pip install -q -U gdown\n",
        "\n",
        "\n",
        "# #@markdown ## **Drive Cookies ‚ö°**\n",
        "# jsonfile = json.loads(drive_cookies)\n",
        "# json_object = json.dumps(jsonfile,indent=1)\n",
        "# with open(\"/root/.cache/gdown/cookies.json\", \"w\") as outfile:\n",
        "#     outfile.write(json_object)\n",
        "\n",
        "def install_aria():\n",
        "  if not os.path.exists('/usr/bin/aria2c'):\n",
        "    !apt -qq install -y aria2\n",
        "\n",
        "\n",
        "#@markdown ###**Explanation**\n",
        "#@markdown - EM: Embeddings (Textual Inversion)\n",
        "#@markdown - HN: Hypernetworks<br><br>\n",
        "\n",
        "\n",
        "#@markdown ## **From Huggingface ü§ó**\n",
        "#@markdown *custom token is optional, you can leave it empty if you want*\n",
        "#@markdown <br>*vae are now included in the models*\n",
        "custom_hf_token = \"\" #@param {type:\"string\"}\n",
        "default_token = \"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\"\n",
        "user_token = custom_hf_token if custom_hf_token else default_token\n",
        "user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "def custom_hf_model(url, checkpoint_name=\"\"):\n",
        "  install_aria()\n",
        "  if checkpoint_name:\n",
        "    # !wget -c --show-progress --header={user_header} {url} -O {root_dir}/stable-diffusion-webui/models/Stable-diffusion/{checkpoint_name}.ckpt\n",
        "    !aria2c  --summary-interval=10 -c --header={user_header} -x 10 -k 1M -s 10 -d {root_dir}/stable-diffusion-webui/models/Stable-diffusion -o {checkpoint_name} {url}\n",
        "  else:\n",
        "    !aria2c  --summary-interval=10 -c --header={user_header} -x 10 -k 1M -s 10 -d {root_dir}/stable-diffusion-webui/models/Stable-diffusion/ {url}\n",
        "\n",
        "def custom_hf_hn(url, checkpoint_name=\"\"):\n",
        "  if checkpoint_name:\n",
        "    !aria2c  --summary-interval=10 -c --header={user_header} -x 10 -k 1M -s 10 -d {root_dir}/stable-diffusion-webui/models/hypernetworks -o {checkpoint_name} {url}\n",
        "  else:\n",
        "    !aria2c  --summary-interval=10 -c --header={user_header} -x 10 -k 1M -s 10 -d {root_dir}/stable-diffusion-webui/models/hypernetworks/ {url}\n",
        "\n",
        "def custom_hf_em(url, checkpoint_name=\"\"):\n",
        "  if checkpoint_name:\n",
        "    !aria2c  --summary-interval=10 -c --header={user_header} -x 10 -k 1M -s 10 -d {root_dir}/stable-diffusion-webui/embeddings -o {checkpoint_name} {url}\n",
        "  else:\n",
        "    !aria2c  --summary-interval=10 -c --header={user_header} -x 10 -k 1M -s 10 -d {root_dir}/stable-diffusion-webui/embeddings/ {url}\n",
        "\n",
        "Waifu_diffusion_1_4_e1 = False #@param {'type':'boolean'}\n",
        "if Waifu_diffusion_1_4_e1:\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e1.ckpt\", \"wd-1-4-anime_e1.ckpt\")\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"wd-1-4-anime_e1.vae.pt\")\n",
        "  !wget https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/wd-1-4-anime_e1.yaml -O {root_dir}/stable-diffusion-webui/models/Stable-diffusion/wd-1-4-anime_e1.yaml\n",
        "\n",
        "#@markdown <b><font color=red>. . Waifu Diffusion 1.4 full is not stable for colab usage. Use Pruned FP16 version below \n",
        "\n",
        "Waifu_diffusion_1_4_e1_fp16 = False #@param {'type':'boolean'}\n",
        "if Waifu_diffusion_1_4_e1_fp16:\n",
        "  custom_hf_model(\"https://huggingface.co/subaqua/_unofficial-WD1.4-fp16-safetensors/resolve/main/wd-1-4-anime_e1-fp16.safetensors\", \"wd-1-4-anime_e1-fp16.safetensors\")\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"wd-1-4-anime_e1-fp16.vae.pt\")\n",
        "  !wget https://huggingface.co/subaqua/_unofficial-WD1.4-fp16-safetensors/resolve/main/wd-1-4-anime_e1-fp16.yaml -O {root_dir}/stable-diffusion-webui/models/Stable-diffusion/wd-1-4-anime_e1-fp16.yaml\n",
        "  \n",
        "Anything_safetensors = True #@param {'type':'boolean'}\n",
        "if Anything_safetensors:\n",
        "  custom_hf_model(\"https://huggingface.co/NoCrypt/safetensor_models/resolve/main/Anything-V3.0-pruned-fp32.safetensors\", \"Anything-V3.0-pruned-fp32.safetensors\")\n",
        "  custom_hf_model(\"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0.vae.pt\", \"Anything-V3.0-pruned-fp32.vae.pt\")\n",
        "\n",
        "Waifu_diffusion_1_3_safetensors = False #@param {'type':'boolean'}\n",
        "if Waifu_diffusion_1_3_safetensors:\n",
        "  custom_hf_model(\"https://huggingface.co/NoCrypt/safetensor_models/resolve/main/wd-v1-3-float32.safetensors\", \"waifu_diffusion_1_3.safetensors\")\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"waifu_diffusion_1_3.vae.pt\")\n",
        "\n",
        "Elysium_Anime_v2_safetensors = False #@param {'type':'boolean'}\n",
        "if Elysium_Anime_v2_safetensors:\n",
        "  custom_hf_model(\"https://huggingface.co/NoCrypt/safetensor_models/resolve/main/Elysium_Anime_V2.safetensors\", \"Elysium_Anime_V2.safetensors\")\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"Elysium_Anime_V2.vae.pt\")\n",
        "\n",
        "Elysium_Anime_v3_safetensors = False #@param {'type':'boolean'}\n",
        "if Elysium_Anime_v3_safetensors:\n",
        "  custom_hf_model(\"https://huggingface.co/hesw23168/SD-Elysium-Model/resolve/main/Elysium_Anime_V3.safetensors\", \"Elysium_Anime_V3.safetensors\")\n",
        "  # Using Waifu-Diffusion VAE since it's recommended\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"Elysium_Anime_V2.vae.pt\")\n",
        "\n",
        "Waifu_diffusion_1_3 = False #@param {'type':'boolean'}\n",
        "if Waifu_diffusion_1_3:\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\", \"waifu_diffusion_1_3.ckpt\")\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"waifu_diffusion_1_3.vae.pt\")\n",
        "\n",
        "Anything = False #@param {'type':'boolean'}\n",
        "if Anything:\n",
        "  custom_hf_model(\"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0-pruned.ckpt\", \"anything.ckpt\")\n",
        "  custom_hf_model(\"https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/Anything-V3.0.vae.pt\", \"anything.vae.pt\")\n",
        "\n",
        "Elysium_Anime_v2 = False #@param {'type':'boolean'}\n",
        "if Elysium_Anime_v2:\n",
        "  custom_hf_model(\"https://huggingface.co/hesw23168/SD-Elysium-Model/resolve/main/Elysium_Anime_V2.ckpt\", \"Elysium_Anime_V2.ckpt\")\n",
        "  # Using Waifu-Diffusion VAE since it's recommended\n",
        "  custom_hf_model(\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt\", \"Elysium_Anime_V2.vae.pt\")\n",
        "\n",
        "f222 = False #@param {'type':'boolean'}\n",
        "if f222:\n",
        "  custom_hf_model(\"https://huggingface.co/acheong08/f222/resolve/main/f222.ckpt\", \"f222.ckpt\")\n",
        "\n",
        "Gyokai = False #@param {'type':'boolean'}\n",
        "if Gyokai:\n",
        "  custom_hf_model(\"https://huggingface.co/NoCrypt/gyokai/resolve/main/gyokai.ckpt\", \"gyokai.ckpt\")\n",
        "\n",
        "midjourney_v4_diffusion = False #@param {'type':'boolean'}\n",
        "if midjourney_v4_diffusion:\n",
        "  custom_hf_model(\"https://huggingface.co/prompthero/midjourney-v4-diffusion/resolve/main/mdjrny-v4.ckpt\", \"midjourney-v4-diffusion.ckpt\")\n",
        "\n",
        "cafe_instagram_unofficial_test = False #@param {'type':'boolean'}\n",
        "if cafe_instagram_unofficial_test:\n",
        "  custom_hf_model(\"https://huggingface.co/NoCrypt/cafe-instagram-unofficial/resolve/main/cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt\", \"cafe-instagram-unofficial-test-epoch-9-140k-images-fp32.ckpt\")\n",
        "\n",
        "cafe_instagram_unofficial_test_v2 = False #@param {'type':'boolean'}\n",
        "if cafe_instagram_unofficial_test_v2:\n",
        "  custom_hf_model(\"https://huggingface.co/cafeai/cafe-instagram-sd-1-5-v6/resolve/main/instagram-latest-plus-clip-v6e1_50000.safetensors\", \"instagram-latest-plus-clip-v6e1_50000.safetensors\")\n",
        "\n",
        "Trinart = False #@param {'type':'boolean'}\n",
        "if Trinart:\n",
        "  custom_hf_model(\"https://huggingface.co/naclbit/trinart_characters_19.2m_stable_diffusion_v1/resolve/main/trinart_characters_it4_v1.ckpt\", \"trinart.ckpt\")\n",
        "  custom_hf_model(\"https://huggingface.co/naclbit/trinart_characters_19.2m_stable_diffusion_v1/resolve/main/autoencoder_fix_kl-f8-trinart_characters.ckpt\", \"trinart.vae.pt\")\n",
        "\n",
        "H_Diffusion_17 = False #@param {'type':'boolean'}\n",
        "if H_Diffusion_17:\n",
        "  custom_hf_model(\"https://huggingface.co/Deltaadams/HentaiDiffusion/resolve/main/HD-17.ckpt\", \"HD-17.ckpt\")\n",
        "\n",
        "SD_V1_4 = False #@param {'type':'boolean'}\n",
        "if SD_V1_4:\n",
        "  custom_hf_model(\"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\", \"stable_diffusion.ckpt\")\n",
        "\n",
        "SD_V1_5 = False #@param {'type':'boolean'}\n",
        "if SD_V1_5:\n",
        "  custom_hf_model(\"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\", \"sd-v1-5.ckpt\")\n",
        "  custom_hf_model(\"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt\",\"sd-v1-5.vae.pt\")\n",
        "\n",
        "SD_v1_5_inpainting = False #@param {'type':'boolean'}\n",
        "if SD_v1_5_inpainting:\n",
        "  custom_hf_model(\"https://huggingface.co/runwayml/stable-diffusion-inpainting/resolve/main/sd-v1-5-inpainting.ckpt\", \"sd-v1-5-inpainting.ckpt\")\n",
        "\n",
        "SD_V2_768 = False #@param {'type':'boolean'}\n",
        "if SD_V2_768:\n",
        "  custom_hf_model(\"https://huggingface.co/stabilityai/stable-diffusion-2/resolve/main/768-v-ema.ckpt\", \"768-v-ema.ckpt\")\n",
        "  !wget https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/v2-inference-v.yaml -O {root_dir}/stable-diffusion-webui/models/Stable-diffusion/768-v-ema.yaml\n",
        "\n",
        "SD_V2_Inpainting = False #@param {'type':'boolean'}\n",
        "if SD_V2_Inpainting:\n",
        "  custom_hf_model(\"https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.ckpt\", \"512-inpainting-ema.ckpt\")\n",
        "  !wget https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/configs/stable-diffusion/v2-inpainting-inference.yaml -O {root_dir}/stable-diffusion-webui/models/Stable-diffusion/512-inpainting-ema.yaml\n",
        "  !sed -i -e '18d' {root_dir}/stable-diffusion-webui/models/Stable-diffusion/512-inpainting-ema.yaml #https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/5008#discussioncomment-4249137\n",
        "\n",
        "custom_url = \"\" #@param {type:\"string\"}\n",
        "if custom_url:\n",
        "  custom_hf_model(custom_url)\n",
        "\n",
        "custom_url_2 = \"\" #@param {type:\"string\"}\n",
        "if custom_url_2:\n",
        "  custom_hf_model(custom_url_2)\n",
        "\n",
        "custom_url_HN = \"\" #@param {type:\"string\"}\n",
        "if custom_url_HN:\n",
        "  custom_hf_hn(custom_url_HN)\n",
        "  \n",
        "custom_url_embedding = \"\" #@param {type:\"string\"}\n",
        "if custom_url_embedding:\n",
        "  custom_hf_em(custom_url_embedding)\n",
        "\n",
        "#------------------#\n",
        "\n",
        "#@markdown ## <br>**From Torrent, Magnet, and Other Direct Link ‚õìÔ∏èüß≤**\n",
        "\n",
        "\n",
        "def direct_model(url):\n",
        "  install_aria()\n",
        "  !aria2c --summary-interval=10 --seed-ratio=0.1 --allow-overwrite=true -d {root_dir}/stable-diffusion-webui/models/Stable-diffusion/ {url}\n",
        "  \n",
        "def direct_hn(url):\n",
        "  install_aria()\n",
        "  !aria2c --summary-interval=10 --seed-ratio=0.1 --allow-overwrite=true -d {root_dir}/stable-diffusion-webui/models/hypernetworks/ {url}\n",
        "\n",
        "def direct_em(url):\n",
        "  install_aria()\n",
        "  !aria2c --summary-interval=10 --seed-ratio=0.1 --allow-overwrite=true -d {root_dir}/stable-diffusion-webui/embeddings/ {url}\n",
        "\n",
        "file_url = \"\" #@param{'type':'string'}\n",
        "if file_url:\n",
        "  direct_model(file_url)\n",
        "\n",
        "file_url_2 = \"\" #@param{'type':'string'}\n",
        "if file_url_2:\n",
        "  direct_model(file_url_2)\n",
        "\n",
        "file_url_HN = \"\" #@param{'type':'string'}\n",
        "if file_url_HN:\n",
        "  direct_hn(file_url_HN)\n",
        "\n",
        "file_url_EM = \"\" #@param{'type':'string'}\n",
        "if file_url_EM:\n",
        "  direct_em(file_url_EM)\n",
        "\n",
        "#------------------#\n",
        "\n",
        "#@markdown ## <br>**From Google Drive Link ‚öôÔ∏è**\n",
        "is_folder_url = False #@param{'type':'boolean'}\n",
        "def drive_link_model(url):\n",
        "  %cd {root_dir}/stable-diffusion-webui/models/Stable-diffusion/\n",
        "  if is_folder_url:\n",
        "    !gdown --fuzzy {url} -c --folder\n",
        "  else:\n",
        "    !gdown --fuzzy {url} -c\n",
        "  %cd\n",
        "\n",
        "def drive_link_hn(url):\n",
        "  %cd {root_dir}/stable-diffusion-webui/models/hypernetworks/\n",
        "  if is_folder_url:\n",
        "    !gdown --fuzzy {url} -c --folder\n",
        "  else:\n",
        "    !gdown --fuzzy {url} -c\n",
        "  %cd {root_dir}/stable-diffusion-webui/models/Stable-diffusion/\n",
        "\n",
        "def drive_link_em(url):\n",
        "  %cd {root_dir}/stable-diffusion-webui/embeddings/\n",
        "  if is_folder_url:\n",
        "    !gdown --fuzzy {url} -c --folder\n",
        "  else:\n",
        "    !gdown --fuzzy {url} -c\n",
        "  %cd {root_dir}/stable-diffusion-webui/models/Stable-diffusion/\n",
        "\n",
        "drive_url = \"\" #@param{'type':'string'}\n",
        "if drive_url:\n",
        "  drive_link_model(drive_url)\n",
        "\n",
        "drive_url_2 = \"\" #@param{'type':'string'}\n",
        "if drive_url_2:\n",
        "  drive_link_model(drive_url_2)\n",
        "\n",
        "drive_url_HN = \"\" #@param{'type':'string'}\n",
        "if drive_url_HN:\n",
        "  drive_link_hn(drive_url_HN)\n",
        "\n",
        "drive_url_EM = \"\" #@param{'type':'string'}\n",
        "if drive_url_EM:\n",
        "  drive_link_em(drive_url_EM)\n",
        "\n",
        "#------------------#\n",
        "\n",
        "#@markdown ## <br>**From Your Google Drive üöã**\n",
        "#@markdown - /content/gdrive/MyDrive/`<the rest here, type it below>`\n",
        "#@markdown - for file, type the path. for folders, make sure to end it with `/.` <br>Example: `NAI/.`\n",
        "\n",
        "def gd_path(url):\n",
        "  if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "  !rsync -avr --progress \"/content/gdrive/MyDrive/{url}\" '/content/stable-diffusion-webui/models/Stable-diffusion' \n",
        "\n",
        "def gd_path_hn(url):\n",
        "  if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "  !rsync -avr --progress \"/content/gdrive/MyDrive/{url}\" '/content/stable-diffusion-webui/models/hypernetworks' \n",
        "\n",
        "def gd_path_em(url):\n",
        "  if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "  !rsync -avr --progress \"/content/gdrive/MyDrive/{url}\" '/content/stable-diffusion-webui/embeddings' \n",
        "\n",
        "drive_path = \"\" #@param{'type':'string'}\n",
        "if drive_path:\n",
        "  gd_path(drive_path)\n",
        "\n",
        "drive_path_2 = \"\" #@param{'type':'string'}\n",
        "if drive_path_2:\n",
        "  gd_path(drive_path_2)\n",
        "\n",
        "drive_path_HN = \"\" #@param{'type':'string'}\n",
        "if drive_path_HN:\n",
        "  gd_path_hn(drive_path_HN)\n",
        "  \n",
        "drive_path_EM = \"\" #@param{'type':'string'}\n",
        "if drive_path_EM:\n",
        "  gd_path_em(drive_path_EM)\n",
        "\n",
        "#------------------#\n",
        "#@markdown ## <br>**Manual upload üì¶**\n",
        "#@markdown Click on the link below to open directory you want, drag and drop your file, rename it, etc..\n",
        "#@markdown - **Models: /content/stable-diffusion-webui/models/Stable-diffusion**\n",
        "#@markdown - **Embeddings: /content/stable-diffusion-webui/embeddings**\n",
        "#@markdown - **Hypernetworks: /content/stable-diffusion-webui/models/hypernetworks**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RkggE82nGRlp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "try:\n",
        "  start_colab\n",
        "except:\n",
        "  sys.exit(\"ü´§ YOU HAVEN'T RUN INSTALL DEPENDENCIES CELL\")\n",
        "#@title # **üöÄ Start webui**\n",
        "#@markdown ### **Select your server**\n",
        "server = \"localtunnel\" #@param [\"bore\",\"cloudflared\",\"localtunnel\",\"gradio\",\"ngrok\"]\n",
        "#@markdown  <font color=\"gray\">. . **Best to worst:** ngrok (need token tho) > cloudflared > bore > localtunnel > gradio (somtimes better)<br>\n",
        "%cd /content\n",
        "# This code is purely ((((ABSURD)))) and tends to broke after gradio update ü•≤ (IF ANYONE KNOW THE ALTERNATIVE PLEASE TELL ME üò≠, without freezing gradio version)\n",
        "!sed -i '/is_colab = /c\\            self.is_colab = False' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "param = \"\"\n",
        "#@markdown &nbsp; &nbsp; Gradio username and password is optional.\n",
        "gradio_username = \"\"#@param {type:\"string\"}\n",
        "gradio_password = \"\"#@param {type:\"string\"}\n",
        "if gradio_username and gradio_password:\n",
        "  param += \" --gradio-auth {}:{}\".format(gradio_username, gradio_password)\n",
        "\n",
        "if server == \"gradio\":\n",
        "  param+=\" --share\"\n",
        "  !sed -i '/self.server_name = /c\\            self.server_name = server_name' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/self.server_port = /c\\            self.server_port = server_port' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/    else \"http/c\\                else \"http\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/\"PUBLIC_SHARE_TRUE\":/c\\    \"PUBLIC_SHARE_TRUE\": \"\u001b[32mConnected\u001b[0m\",' /usr/local/lib/python3.8/dist-packages/gradio/strings.py\n",
        "\n",
        "elif server == \"localtunnel\":\n",
        "  if not os.path.exists('/tools/node/bin/lt'):\n",
        "    !npm install -g localtunnel\n",
        "  !nohup lt --port 7860 > srv.txt 2>&1 &\n",
        "  time.sleep(2)\n",
        "  !grep -o 'https[^ ]*' /content/srv.txt >srvr.txt\n",
        "  time.sleep(2)\n",
        "  srv= getoutput('cat /content/srvr.txt')\n",
        "  !sed -i '/self.server_name = /c\\            self.server_name = \"{srv[8:]}\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/self.server_port = /c\\            self.server_port = 443' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/    else \"http/c\\                else \"https\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/\"PUBLIC_SHARE_TRUE\":/c\\    \"PUBLIC_SHARE_TRUE\": \"\u001b[32mConnected\u001b[0m\",' /usr/local/lib/python3.8/dist-packages/gradio/strings.py\n",
        "  !rm /content/srv.txt\n",
        "  !rm /content/srvr.txt\n",
        "\n",
        "elif server == \"cloudflared\":\n",
        "  if not os.path.exists('/usr/bin/cloudflared'):\n",
        "    !curl -Lo /usr/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared\n",
        "  #keep trying untill get one! sorry cloudflare api üòÇ\n",
        "  while True:\n",
        "    !cloudflared tunnel --url localhost:7860 > clf.txt 2>&1 &\n",
        "    time.sleep(9)\n",
        "    !grep -m2 -o 'https[^ ]*' /content/clf.txt | tail -n1 > clfr.txt\n",
        "    srvs = getoutput('cat /content/clf.txt')\n",
        "    if \"has been created\" in srvs:\n",
        "      break\n",
        "    !rm /content/clf.txt\n",
        "    !rm /content/clfr.txt\n",
        "  srv= getoutput('cat /content/clfr.txt')\n",
        "  !sed -i '/self.server_name = /c\\            self.server_name = \"{srv[8:]}\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/self.server_port = /c\\            self.server_port = 443' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/    else \"http/c\\                else \"https\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/\"PUBLIC_SHARE_TRUE\":/c\\    \"PUBLIC_SHARE_TRUE\": \"\u001b[32mConnected\u001b[0m\",' /usr/local/lib/python3.8/dist-packages/gradio/strings.py\n",
        "  !rm /content/clf.txt\n",
        "  !rm /content/clfr.txt\n",
        "\n",
        "elif server == \"bore\":\n",
        "  if not os.path.exists('/usr/bin/bore'):\n",
        "    !curl -Ls https://github.com/ekzhang/bore/releases/download/v0.4.0/bore-v0.4.0-x86_64-unknown-linux-musl.tar.gz | tar zx -C /usr/bin\n",
        "  !bore local 7860 --to bore.pub > bore.txt 2>&1 &\n",
        "  time.sleep(2)\n",
        "  !grep -o -m1 'bore.pub:[^ ]*' /content/bore.txt > boreport.txt\n",
        "  boreport = getoutput('cat /content/boreport.txt')\n",
        "  !sed -i '/self.server_name = /c\\            self.server_name = \"bore.pub\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/self.server_port = /c\\            self.server_port = {boreport[9:]}' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/    else \"http/c\\                else \"http\"' /usr/local/lib/python3.8/dist-packages/gradio/blocks.py\n",
        "  !sed -i '/\"PUBLIC_SHARE_TRUE\":/c\\    \"PUBLIC_SHARE_TRUE\": \"\u001b[32mConnected, using HTTP only!!\u001b[0m\",' /usr/local/lib/python3.8/dist-packages/gradio/strings.py\n",
        "  !rm /content/bore.txt\n",
        "  !rm /content/boreport.txt\n",
        "\n",
        "# Model\n",
        "first_load_model = \"auto\" #@param [\"auto\", \"anything.ckpt\", \"waifu_diffusion.ckpt\", \"gyokai.ckpt\", \"Elysium_Anime_V2.ckpt\", \"trinart.ckpt\", \"HD-17.ckpt\", \"stable_diffusion.ckpt\", \"sd-v1-5.ckpt\", \"sd-v1-5-inpainting.ckpt\" ] {allow-input: true}\n",
        "param += \" --ckpt \"+root_dir+\"/stable-diffusion-webui/models/Stable-diffusion/\"+first_load_model if first_load_model!=\"auto\" else \"\"\n",
        "\n",
        "#@markdown If you want to use hypernetworks, please turn off the xformers\n",
        "# XFormers\n",
        "xformers = True #@param {type:\"boolean\"}\n",
        "if xformers:\n",
        "  s = getoutput('nvidia-smi --query-gpu=gpu_name --format=csv')\n",
        "  #Check whether it's already installed or not\n",
        "  if os.path.exists('/usr/local/lib/python3.8/dist-packages/xformers'):\n",
        "    #Change xformers if it's A100\n",
        "    if \"A100\" in s:\n",
        "      !wget -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/A100\n",
        "      %cd /usr/local/lib/python3.8/dist-packages/xformers\n",
        "      !7z x -y /content/A100\n",
        "      !rm /content/A100\n",
        "    if not (\"T4\" in s or \"A100\" in s):\n",
        "      print(\"[XFORMERS] GPU isn't supported.\")\n",
        "      !pip uninstall -q -y xformers\n",
        "      xformers = False\n",
        "  else:\n",
        "    if 'T4' in s:\n",
        "      # !pip install -U --pre triton\n",
        "      !pip install -q https://github.com/camenduru/stable-diffusion-webui-colab/releases/download/0.0.15/xformers-0.0.15.dev0+189828c.d20221207-cp38-cp38-linux_x86_64.whl\n",
        "    elif \"A100\" in s:\n",
        "      !wget -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/A100\n",
        "      %cd /usr/local/lib/python3.8/dist-packages/xformers\n",
        "      !7z x -y /content/A100\n",
        "      !rm /content/A100\n",
        "    else:\n",
        "      print(\"[XFORMERS] GPU isn't supported.\")\n",
        "      xformers = False\n",
        "param += \" --xformers\" if xformers else \"\"\n",
        "s=\"\" #save ram, by few bytes, lol\n",
        "\n",
        "# Disable safe unpickle\n",
        "disable_safe_unpickle = True #@param{\"type\":\"boolean\"}\n",
        "param += \" --disable-safe-unpickle\" if disable_safe_unpickle else \"\"\n",
        "\n",
        "# Medvram\n",
        "medvram = False #@param{\"type\":\"boolean\"}\n",
        "param += \" --medvram\" if medvram else \"\"\n",
        "\n",
        "# precision_full_no_half\n",
        "precision_full_no_half = False #@param{\"type\":\"boolean\"}\n",
        "param += \" --precision full --no-half\" if precision_full_no_half else \"\"\n",
        "\n",
        "# API\n",
        "api = False #@param{\"type\":\"boolean\"}\n",
        "paint_hua_support = False #@param{\"type\":\"boolean\"}\n",
        "param += \" --api --cors-allow-origins=https://www.painthua.com\" if paint_hua_support else \" --api\" if api else \"\"\n",
        "\n",
        "# No Half VAE\n",
        "no_half_vae = True #@param{\"type\":\"boolean\"}\n",
        "param += \" --no-half-vae\" if no_half_vae else \"\"\n",
        "\n",
        "img2img_color_sketch = False #@param{\"type\":\"boolean\"}\n",
        "param+=\" --gradio-img2img-tool color-sketch\" if img2img_color_sketch else \"\"\n",
        "\n",
        "inpaint_color_sketch = False #@param{\"type\":\"boolean\"}\n",
        "param+=\" --gradio-inpaint-tool color-sketch\" if inpaint_color_sketch else \"\"\n",
        "\n",
        "dark_theme = True #@param{\"type\":\"boolean\"}\n",
        "param+=\" --theme dark\" if dark_theme else \" --theme light\"\n",
        "\n",
        "\n",
        "# Custom arguments last\n",
        "custom_arguments = \"\" #@param{\"type\":\"string\"}\n",
        "param += \" \" + custom_arguments if custom_arguments else \"\"\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### **ngrok credentials (required if you use ngrok)**\n",
        "#@markdown [Get ngrok token here](https://dashboard.ngrok.com/get-started/your-authtoken) || [Region explaination](https://ngrok.com/docs/ngrok-agent/config#region)\n",
        "\n",
        "ngrok_token = \"\" #@param{type:\"string\"}\n",
        "ngrok_region = \"ap\" #@param [\"us\", \"eu\", \"au\", \"ap\", \"sa\", \"jp\", \"in\"]\n",
        "#@markdown Tired of copy pasting ngrok token everytime? Me too, let's fix that:\n",
        "save_and_use_token_and_region_in_drive = True #@param{type:\"boolean\"}\n",
        "#@markdown This option will write token from above, please leave it on, and leave all empty/default next time you use this\n",
        "if server == \"ngrok\":\n",
        "  if save_and_use_token_and_region_in_drive:\n",
        "    if not os.path.exists('/content/gdrive'):\n",
        "      drive.mount('/content/gdrive')\n",
        "    if ngrok_token:\n",
        "      if not os.path.exists(\"/content/gdrive/MyDrive/WebUI/ngrokToken.txt\"):\n",
        "        !mkdir -p /content/gdrive/MyDrive/WebUI/\n",
        "        !touch /content/gdrive/MyDrive/WebUI/ngrokToken.txt\n",
        "      f = open(\"/content/gdrive/MyDrive/WebUI/ngrokToken.txt\", \"w+\")\n",
        "      f.write(ngrok_token+\",\"+ngrok_region)\n",
        "      f.close()\n",
        "    elif os.path.exists('/content/gdrive/MyDrive/WebUI/ngrokToken.txt'):\n",
        "      ngrok_token,ngrok_region = getoutput(\"cat /content/gdrive/MyDrive/WebUI/ngrokToken.txt\").split(\",\",2)\n",
        "    else:\n",
        "      print(\"warning: ngrok token not detected\")\n",
        "  param+=' --ngrok '+ngrok_token+' --ngrok-region '+ngrok_region\n",
        "\n",
        "\n",
        "%cd {root_dir}/stable-diffusion-webui\n",
        "def colab_time():\n",
        "  import time\n",
        "  from datetime import timedelta\n",
        "  time_since_start = timedelta(seconds=time.time()-start_colab)\n",
        "  if time_since_start.seconds / 60 < 45:\n",
        "    print(\"\\033[92m\")\n",
        "  elif time_since_start.seconds / 60 > 80:\n",
        "    print(\"\\033[91m\")\n",
        "  else:\n",
        "    print(\"\\033[93m\")\n",
        "  print(\"\\n\\n=======================================================\\n\\n\")\n",
        "  print(\"‚åö You've been running this colab for:\",\"%02d:%02d:%02d\" % (time_since_start.seconds / 3600, (time_since_start.seconds / 60) % 60, time_since_start.seconds % 60),\" ‚åö\")\n",
        "  print(\"\\n\\n=======================================================\\n\\n\\033[0m\")\n",
        "\n",
        "colab_time()\n",
        "if using_experimental:\n",
        "  print(\"\\033[93m‚ö†Ô∏è WARNING: EXPERIMENTAL OPTION IS ACTIVE! ‚ö†Ô∏è\\nDO NOT REPORT IF ANYTHING IS BROKEN!\\nThe output will be yellow during startup to mark this\\nTo Remove, Rerun install dep without experimental, but restart runtime is recommended\\n\")\n",
        "!COMMANDLINE_ARGS=\"\" REQS_FILE=\"requirements.txt\" python launch.py $param\n",
        "colab_time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6B9e8Zxoj1N"
      },
      "source": [
        "# **‚öíÔ∏è Advanced Stuff**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B977dVS6AZcL"
      },
      "outputs": [],
      "source": [
        "#@title # ‚ö†Ô∏è ***(ONLY RUN IF BROKEN)*** Run this if things broken\n",
        "import os\n",
        "\n",
        "nocrypt_said_enable_this = False #@param{type:'boolean'}\n",
        "if nocrypt_said_enable_this:\n",
        "  import json\n",
        "  from types import FunctionType, ModuleType\n",
        "  var_names = dir()\n",
        "  var_dict = {}\n",
        "  exclusion_list = [\n",
        "     \"url\",\"payload\", \"data\",\"var_dict\", \"var_names\", \"exclusion_list\", \"In\", \"Out\", '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_exit_code', '_i', '_i1', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i19', '_i2', '_i20', '_i21', '_i3', '_i4', '_i5', '_i6', '_i7', '_i8', '_i9', '_ih', '_ii', '_iii', '_oh'\n",
        "  ]\n",
        "  for name in var_names:\n",
        "    if name not in exclusion_list:\n",
        "      lc = locals()[name]\n",
        "      if not isinstance(lc, (FunctionType, ModuleType)):\n",
        "        # Check if the item can be converted to JSON\n",
        "        try:\n",
        "            json.dumps(lc)\n",
        "        except TypeError:\n",
        "            # If the item cannot be converted to JSON, skip it\n",
        "            continue\n",
        "        var_dict[name] = lc\n",
        "  import requests\n",
        "  url = \"https://hastebin.com/documents\"\n",
        "  payload = str(var_dict)\n",
        "  headers = {\"content-type\": \"text/plain\"}\n",
        "  response = requests.request(\"POST\", url, data=payload, headers=headers)\n",
        "  if response.ok:\n",
        "    print(\"Now copy this link, and send it to me for further inspection:\")\n",
        "    print(\"https://hastebin.com/raw/\"+response.json()[\"key\"])\n",
        "  else:\n",
        "    print(\"error:\", response.status_code, response.text)\n",
        "\n",
        "\n",
        "#@markdown Please rerun \"Install Depedencies\" after running anything below.\n",
        "\n",
        "\n",
        "remove_repos = False #@param{type:'boolean'}\n",
        "if remove_repos: \n",
        "  # This will delete all repositories, just in case if its corrupted \n",
        "  !rm -rf {root_dir}/stable-diffusion-webui/repositories/ \n",
        "\n",
        "cleanup_experimentals = False #@param{type:'boolean'}\n",
        "if cleanup_experimentals:\n",
        "  # Just in case if you've used experimental options\n",
        "  !git checkout master\n",
        "  # !git branch --delete experimental_nc\n",
        "  !git reset --hard origin/master\n",
        "  !git pull\n",
        "\n",
        "remove_everything = False #@param{type:'boolean'}\n",
        "if remove_everything:\n",
        "  # Self-explanatory\n",
        "  !rm -rf {root_dir}/stable-diffusion-webui\n",
        "  del start_colab\n",
        "\n",
        "uninstall_xformers = False #@param{type:'boolean'}\n",
        "if uninstall_xformers:\n",
        "  # Self-explanatory, useful if you want to try hypernetworks\n",
        "  !pip uninstall xformers\n",
        "\n",
        "restart_runtime = False #@param{type:'boolean'}\n",
        "if restart_runtime:\n",
        "  # This will crash Colab (required if you want to free up ram n vram)\n",
        "  os.kill(os.getpid(), 9) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xSLziGY-sa89"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "models_path = root_dir+\"/stable-diffusion-webui/models/Stable-diffusion\"\n",
        "upload_path = '/content/upload_models'\n",
        "\n",
        "#@title # **üëá Upload (merged) models to HuggingFace üöÄü§ó**\n",
        "#@markdown ## **How to use this?**<br>\n",
        "#@markdown 1. Grab your huggingface **write token** from [here](https://huggingface.co/settings/tokens)\n",
        "#@markdown 2. Paste the token, then write your repo name\n",
        "#@markdown 3. Run the cell\n",
        "#@markdown 4. Select model you want to upload (use `ctrl/shift` for multiple selection)\n",
        "#@markdown 5. Click on upload button\n",
        "\n",
        "api = HfApi()\n",
        "write_token = \"\"  #@param{type:\"string\"}\n",
        "if not os.path.exists('/root/.huggingface/token'):\n",
        "  !mkdir /root/.huggingface/\n",
        "  !touch /root/.huggingface/token\n",
        "f = open(\"/root/.huggingface/token\", \"w+\")\n",
        "f.write(write_token)\n",
        "f.close()\n",
        "\n",
        "user = api.whoami(write_token)\n",
        "repo = \"models\" #@param{type:\"string\"}\n",
        "username_repo = user['name']+\"/\"+repo.strip()\n",
        "validate_repo_id(username_repo)\n",
        "\n",
        "try:\n",
        "  api.create_repo(repo_id=username_repo)\n",
        "  print(\"Repo didn't exists, creating repo\")\n",
        "  print(\"Repo\",username_repo,\"created!\")\n",
        "except HfHubHTTPError as e:\n",
        "  print(\"Repo exists, skipping create repo\")\n",
        "  \n",
        "#@markdown ####<br> **‚ö†Ô∏è Rerun this cell if your colab crashed. Because sometimes it did ü´§**<br>\n",
        "#@markdown *If error, maybe you have \"read token\" instead of \"write token\"*\n",
        "def get_file_list(path):\n",
        "  res = []\n",
        "  for (dir_path, dir_names, file_names) in os.walk(path):\n",
        "      res.extend(file_names)\n",
        "  return res\n",
        "  \n",
        "selected = widgets.SelectMultiple(\n",
        "    options=get_file_list(models_path),\n",
        "    rows=10,\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Upload',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Upload to huggingface',\n",
        ")\n",
        "\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "def upload_it(b):\n",
        "  with out:\n",
        "    if selected.value is not None:\n",
        "      clear_output()\n",
        "      !mkdir -p {upload_path}\n",
        "\n",
        "      #hard link each file\n",
        "      for selected_model in selected.value:\n",
        "        if not os.path.exists(os.path.join(upload_path,selected_model)):\n",
        "          os.link(os.path.join(models_path,selected_model),os.path.join(upload_path,selected_model)) #hardlinking to save colab's space\n",
        "      \n",
        "      #delete .ipynb_checkpoint\n",
        "      if os.path.exists(os.path.join(upload_path,\".ipynb_checkpoints\")):\n",
        "        !rm {upload_path}/.ipynb_checkpoints\n",
        "      print(\"Selected:\", \", \".join(selected.value))\n",
        "      print(\"Uploading to https://huggingface.co/\"+username_repo)\n",
        "      print(\"Please wait...\")\n",
        "\n",
        "      #upload\n",
        "      api.upload_folder(\n",
        "          folder_path=upload_path,\n",
        "          repo_id=username_repo,\n",
        "          commit_message=\"Upload with üöÄü§ó NoCrypt's sd_webui_colab\"\n",
        "      )\n",
        "      \n",
        "      print(\"Done!\")\n",
        "      #delete hardlink\n",
        "      !rm -rf {upload_path}/*\n",
        "    else:\n",
        "      print(\"Nothing is selected\")\n",
        "\n",
        "button.on_click(upload_it)\n",
        "print(\"Upload target: https://huggingface.co/\"+username_repo)\n",
        "print(\"üëá Select models you want to upload (use ctrl/shift for multiple selection) \")\n",
        "display(selected,button,out)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "c6B9e8Zxoj1N"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
